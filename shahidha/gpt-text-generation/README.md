# Text Generation using GPT-2 (Small)

## Objective
Fine-tune GPT-2 small on a free text corpus and generate paragraphs from prompts.

## Dataset
- Wikipedia (wikitext-2-raw-v1)

## Model
- GPT-2 small (124M parameters)

## How to Run
1. Install dependencies
   pip install -r requirements.txt

2. Train the model
   python train.py

3. Generate text
   python generate.py

## Output
- Fine-tuned GPT-2 model
- Paragraph generation based on prompts
